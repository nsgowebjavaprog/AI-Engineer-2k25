{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement spark-excel (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\nagar\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for spark-excel\n"
     ]
    }
   ],
   "source": [
    "pip install spark-excel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------+\n",
      "|Name| Age|  Ex|Salary|\n",
      "+----+----+----+------+\n",
      "|   A|  11|   2| 30000|\n",
      "|   B|  22|   3| 23232|\n",
      "|   C|  33|   1|232323|\n",
      "|   D|  44|   5| 23434|\n",
      "|   E|  55|   3|    23|\n",
      "|   F|  66|   6|  2354|\n",
      "|   G|  77|   8|     6|\n",
      "|   H|NULL|NULL|  3436|\n",
      "|NULL|  99|   6|    54|\n",
      "|NULL|  45|NULL|  NULL|\n",
      "+----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.csv('III_Handling.csv', header = True, inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------+\n",
      "|Name| Age|  Ex|Salary|\n",
      "+----+----+----+------+\n",
      "|   A|  11|   2| 30000|\n",
      "|   B|  22|   3| 23232|\n",
      "|   C|  33|   1|232323|\n",
      "|   D|  44|   5| 23434|\n",
      "|   E|  55|   3|    23|\n",
      "|   F|  66|   6|  2354|\n",
      "|   G|  77|   8|     6|\n",
      "|   H|NULL|NULL|  3436|\n",
      "|NULL|  99|   6|    54|\n",
      "|NULL|  45|NULL|  NULL|\n",
      "+----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv = spark.read.csv(\"III_Handling.csv\", header=True, inferSchema=True)\n",
    "df_csv.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.drop(\"Name\")\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the row which one having null value\n",
    "\n",
    "df_spark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## any == how\n",
    "\n",
    "# if all are null then only not visible if 1 val then return in output\n",
    "\n",
    "df_spark.on.drop(how = 'any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh = 2 -->  all least 2 val not null\n",
    "\n",
    "df_spark.on.drop(how = 'any',  thresh = 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset --> remove when Ex is null\n",
    "\n",
    "df_spark.on.drop(how = 'any', subset = ['Ex']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling a missing Value\n",
    "\n",
    "df_spark.na.fill('Replace_as_Missing_Value').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling a missing Value when specific col's\n",
    "\n",
    "df_spark.na.fill('Replace_as_Missing_Value', ['EX', 'Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
